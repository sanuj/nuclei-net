I1022 08:26:01.988214  3319 caffe.cpp:184] Using GPUs 0
I1022 08:26:02.177538  3319 solver.cpp:54] Initializing solver from parameters: 
test_iter: 200
test_interval: 1000
base_lr: 0.001
display: 100
max_iter: 40000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 20000
snapshot: 10000
snapshot_prefix: "examples/nuclei/train_cifar/use_cifar_4"
solver_mode: GPU
device_id: 0
net: "examples/nuclei/train_cifar/use_cifar_train_test.prototxt"
I1022 08:26:02.177682  3319 solver.cpp:97] Creating training net from net file: examples/nuclei/train_cifar/use_cifar_train_test.prototxt
I1022 08:26:02.178011  3319 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer nuclei
I1022 08:26:02.178035  3319 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1022 08:26:02.178153  3319 net.cpp:50] Initializing net from parameters: 
name: "NULCEI_quick"
state {
  phase: TRAIN
}
layer {
  name: "nuclei"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  image_data_param {
    source: "/home/sanuj/temp_63_LLM_YR4_33/train.txt"
    batch_size: 200
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 48
    pad: 0
    kernel_size: 6
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 48
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip_1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 20
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip_2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1022 08:26:02.178241  3319 layer_factory.hpp:76] Creating layer nuclei
I1022 08:26:02.178287  3319 net.cpp:110] Creating Layer nuclei
I1022 08:26:02.178300  3319 net.cpp:433] nuclei -> data
I1022 08:26:02.178328  3319 net.cpp:433] nuclei -> label
I1022 08:26:02.178351  3319 image_data_layer.cpp:37] Opening file /home/sanuj/temp_63_LLM_YR4_33/train.txt
I1022 08:26:02.266170  3319 image_data_layer.cpp:52] A total of 199996 images.
I1022 08:26:02.272606  3319 image_data_layer.cpp:79] output data size: 200,3,33,33
I1022 08:26:02.282320  3319 net.cpp:155] Setting up nuclei
I1022 08:26:02.282356  3319 net.cpp:163] Top shape: 200 3 33 33 (653400)
I1022 08:26:02.282366  3319 net.cpp:163] Top shape: 200 (200)
I1022 08:26:02.282377  3319 layer_factory.hpp:76] Creating layer conv1
I1022 08:26:02.282409  3319 net.cpp:110] Creating Layer conv1
I1022 08:26:02.282418  3319 net.cpp:477] conv1 <- data
I1022 08:26:02.282438  3319 net.cpp:433] conv1 -> conv1
I1022 08:26:02.283633  3319 net.cpp:155] Setting up conv1
I1022 08:26:02.283655  3319 net.cpp:163] Top shape: 200 48 28 28 (7526400)
I1022 08:26:02.283704  3319 layer_factory.hpp:76] Creating layer pool1
I1022 08:26:02.283721  3319 net.cpp:110] Creating Layer pool1
I1022 08:26:02.283728  3319 net.cpp:477] pool1 <- conv1
I1022 08:26:02.283740  3319 net.cpp:433] pool1 -> pool1
I1022 08:26:02.283915  3319 net.cpp:155] Setting up pool1
I1022 08:26:02.283934  3319 net.cpp:163] Top shape: 200 48 14 14 (1881600)
I1022 08:26:02.283943  3319 layer_factory.hpp:76] Creating layer relu1
I1022 08:26:02.283953  3319 net.cpp:110] Creating Layer relu1
I1022 08:26:02.283962  3319 net.cpp:477] relu1 <- pool1
I1022 08:26:02.283973  3319 net.cpp:419] relu1 -> pool1 (in-place)
I1022 08:26:02.283985  3319 net.cpp:155] Setting up relu1
I1022 08:26:02.283994  3319 net.cpp:163] Top shape: 200 48 14 14 (1881600)
I1022 08:26:02.284000  3319 layer_factory.hpp:76] Creating layer conv2
I1022 08:26:02.284015  3319 net.cpp:110] Creating Layer conv2
I1022 08:26:02.284024  3319 net.cpp:477] conv2 <- pool1
I1022 08:26:02.284034  3319 net.cpp:433] conv2 -> conv2
I1022 08:26:02.287261  3319 net.cpp:155] Setting up conv2
I1022 08:26:02.287284  3319 net.cpp:163] Top shape: 200 48 11 11 (1161600)
I1022 08:26:02.287302  3319 layer_factory.hpp:76] Creating layer relu2
I1022 08:26:02.287317  3319 net.cpp:110] Creating Layer relu2
I1022 08:26:02.287333  3319 net.cpp:477] relu2 <- conv2
I1022 08:26:02.287343  3319 net.cpp:419] relu2 -> conv2 (in-place)
I1022 08:26:02.287364  3319 net.cpp:155] Setting up relu2
I1022 08:26:02.287374  3319 net.cpp:163] Top shape: 200 48 11 11 (1161600)
I1022 08:26:02.287380  3319 layer_factory.hpp:76] Creating layer pool2
I1022 08:26:02.287394  3319 net.cpp:110] Creating Layer pool2
I1022 08:26:02.287401  3319 net.cpp:477] pool2 <- conv2
I1022 08:26:02.287410  3319 net.cpp:433] pool2 -> pool2
I1022 08:26:02.287462  3319 net.cpp:155] Setting up pool2
I1022 08:26:02.287474  3319 net.cpp:163] Top shape: 200 48 6 6 (345600)
I1022 08:26:02.287482  3319 layer_factory.hpp:76] Creating layer ip_1
I1022 08:26:02.287497  3319 net.cpp:110] Creating Layer ip_1
I1022 08:26:02.287505  3319 net.cpp:477] ip_1 <- pool2
I1022 08:26:02.287515  3319 net.cpp:433] ip_1 -> ip1
I1022 08:26:02.288470  3319 net.cpp:155] Setting up ip_1
I1022 08:26:02.288481  3319 net.cpp:163] Top shape: 200 20 (4000)
I1022 08:26:02.288496  3319 layer_factory.hpp:76] Creating layer relu1
I1022 08:26:02.288511  3319 net.cpp:110] Creating Layer relu1
I1022 08:26:02.288518  3319 net.cpp:477] relu1 <- ip1
I1022 08:26:02.288542  3319 net.cpp:419] relu1 -> ip1 (in-place)
I1022 08:26:02.288557  3319 net.cpp:155] Setting up relu1
I1022 08:26:02.288566  3319 net.cpp:163] Top shape: 200 20 (4000)
I1022 08:26:02.288574  3319 layer_factory.hpp:76] Creating layer ip_2
I1022 08:26:02.288586  3319 net.cpp:110] Creating Layer ip_2
I1022 08:26:02.288594  3319 net.cpp:477] ip_2 <- ip1
I1022 08:26:02.288601  3319 net.cpp:433] ip_2 -> ip2
I1022 08:26:02.288707  3319 net.cpp:155] Setting up ip_2
I1022 08:26:02.288717  3319 net.cpp:163] Top shape: 200 2 (400)
I1022 08:26:02.288728  3319 layer_factory.hpp:76] Creating layer loss
I1022 08:26:02.288744  3319 net.cpp:110] Creating Layer loss
I1022 08:26:02.288753  3319 net.cpp:477] loss <- ip2
I1022 08:26:02.288760  3319 net.cpp:477] loss <- label
I1022 08:26:02.288771  3319 net.cpp:433] loss -> loss
I1022 08:26:02.288787  3319 layer_factory.hpp:76] Creating layer loss
I1022 08:26:02.288887  3319 net.cpp:155] Setting up loss
I1022 08:26:02.288897  3319 net.cpp:163] Top shape: (1)
I1022 08:26:02.288902  3319 net.cpp:168]     with loss weight 1
I1022 08:26:02.288928  3319 net.cpp:236] loss needs backward computation.
I1022 08:26:02.288941  3319 net.cpp:236] ip_2 needs backward computation.
I1022 08:26:02.288949  3319 net.cpp:236] relu1 needs backward computation.
I1022 08:26:02.288956  3319 net.cpp:236] ip_1 needs backward computation.
I1022 08:26:02.288962  3319 net.cpp:236] pool2 needs backward computation.
I1022 08:26:02.288969  3319 net.cpp:236] relu2 needs backward computation.
I1022 08:26:02.288975  3319 net.cpp:236] conv2 needs backward computation.
I1022 08:26:02.288981  3319 net.cpp:236] relu1 needs backward computation.
I1022 08:26:02.289002  3319 net.cpp:236] pool1 needs backward computation.
I1022 08:26:02.289011  3319 net.cpp:236] conv1 needs backward computation.
I1022 08:26:02.289021  3319 net.cpp:240] nuclei does not need backward computation.
I1022 08:26:02.289027  3319 net.cpp:283] This network produces output loss
I1022 08:26:02.289041  3319 net.cpp:297] Network initialization done.
I1022 08:26:02.289047  3319 net.cpp:298] Memory required for data: 58481604
I1022 08:26:02.289363  3319 solver.cpp:187] Creating test net (#0) specified by net file: examples/nuclei/train_cifar/use_cifar_train_test.prototxt
I1022 08:26:02.289398  3319 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer nuclei
I1022 08:26:02.289496  3319 net.cpp:50] Initializing net from parameters: 
name: "NULCEI_quick"
state {
  phase: TEST
}
layer {
  name: "nuclei"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  image_data_param {
    source: "/home/sanuj/temp_63_LLM_YR4_33/test.txt"
    batch_size: 500
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 48
    pad: 0
    kernel_size: 6
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 48
    pad: 0
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip_1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 20
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip_2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1022 08:26:02.289590  3319 layer_factory.hpp:76] Creating layer nuclei
I1022 08:26:02.289608  3319 net.cpp:110] Creating Layer nuclei
I1022 08:26:02.289618  3319 net.cpp:433] nuclei -> data
I1022 08:26:02.289631  3319 net.cpp:433] nuclei -> label
I1022 08:26:02.289644  3319 image_data_layer.cpp:37] Opening file /home/sanuj/temp_63_LLM_YR4_33/test.txt
I1022 08:26:02.337589  3319 image_data_layer.cpp:52] A total of 100000 images.
I1022 08:26:02.337848  3319 image_data_layer.cpp:79] output data size: 500,3,33,33
I1022 08:26:02.361310  3319 net.cpp:155] Setting up nuclei
I1022 08:26:02.361345  3319 net.cpp:163] Top shape: 500 3 33 33 (1633500)
I1022 08:26:02.361353  3319 net.cpp:163] Top shape: 500 (500)
I1022 08:26:02.361362  3319 layer_factory.hpp:76] Creating layer label_nuclei_1_split
I1022 08:26:02.361383  3319 net.cpp:110] Creating Layer label_nuclei_1_split
I1022 08:26:02.361392  3319 net.cpp:477] label_nuclei_1_split <- label
I1022 08:26:02.361402  3319 net.cpp:433] label_nuclei_1_split -> label_nuclei_1_split_0
I1022 08:26:02.361438  3319 net.cpp:433] label_nuclei_1_split -> label_nuclei_1_split_1
I1022 08:26:02.361491  3319 net.cpp:155] Setting up label_nuclei_1_split
I1022 08:26:02.361501  3319 net.cpp:163] Top shape: 500 (500)
I1022 08:26:02.361510  3319 net.cpp:163] Top shape: 500 (500)
I1022 08:26:02.361526  3319 layer_factory.hpp:76] Creating layer conv1
I1022 08:26:02.361541  3319 net.cpp:110] Creating Layer conv1
I1022 08:26:02.361548  3319 net.cpp:477] conv1 <- data
I1022 08:26:02.361562  3319 net.cpp:433] conv1 -> conv1
I1022 08:26:02.361980  3319 net.cpp:155] Setting up conv1
I1022 08:26:02.361994  3319 net.cpp:163] Top shape: 500 48 28 28 (18816000)
I1022 08:26:02.362009  3319 layer_factory.hpp:76] Creating layer pool1
I1022 08:26:02.362022  3319 net.cpp:110] Creating Layer pool1
I1022 08:26:02.362028  3319 net.cpp:477] pool1 <- conv1
I1022 08:26:02.362038  3319 net.cpp:433] pool1 -> pool1
I1022 08:26:02.362085  3319 net.cpp:155] Setting up pool1
I1022 08:26:02.362095  3319 net.cpp:163] Top shape: 500 48 14 14 (4704000)
I1022 08:26:02.362100  3319 layer_factory.hpp:76] Creating layer relu1
I1022 08:26:02.362109  3319 net.cpp:110] Creating Layer relu1
I1022 08:26:02.362115  3319 net.cpp:477] relu1 <- pool1
I1022 08:26:02.362123  3319 net.cpp:419] relu1 -> pool1 (in-place)
I1022 08:26:02.362146  3319 net.cpp:155] Setting up relu1
I1022 08:26:02.362154  3319 net.cpp:163] Top shape: 500 48 14 14 (4704000)
I1022 08:26:02.362159  3319 layer_factory.hpp:76] Creating layer conv2
I1022 08:26:02.362169  3319 net.cpp:110] Creating Layer conv2
I1022 08:26:02.362174  3319 net.cpp:477] conv2 <- pool1
I1022 08:26:02.362182  3319 net.cpp:433] conv2 -> conv2
I1022 08:26:02.366935  3319 net.cpp:155] Setting up conv2
I1022 08:26:02.366986  3319 net.cpp:163] Top shape: 500 48 11 11 (2904000)
I1022 08:26:02.367022  3319 layer_factory.hpp:76] Creating layer relu2
I1022 08:26:02.367050  3319 net.cpp:110] Creating Layer relu2
I1022 08:26:02.367075  3319 net.cpp:477] relu2 <- conv2
I1022 08:26:02.367100  3319 net.cpp:419] relu2 -> conv2 (in-place)
I1022 08:26:02.367126  3319 net.cpp:155] Setting up relu2
I1022 08:26:02.367153  3319 net.cpp:163] Top shape: 500 48 11 11 (2904000)
I1022 08:26:02.367174  3319 layer_factory.hpp:76] Creating layer pool2
I1022 08:26:02.367199  3319 net.cpp:110] Creating Layer pool2
I1022 08:26:02.367223  3319 net.cpp:477] pool2 <- conv2
I1022 08:26:02.367266  3319 net.cpp:433] pool2 -> pool2
I1022 08:26:02.367338  3319 net.cpp:155] Setting up pool2
I1022 08:26:02.367367  3319 net.cpp:163] Top shape: 500 48 6 6 (864000)
I1022 08:26:02.367390  3319 layer_factory.hpp:76] Creating layer ip_1
I1022 08:26:02.367420  3319 net.cpp:110] Creating Layer ip_1
I1022 08:26:02.367444  3319 net.cpp:477] ip_1 <- pool2
I1022 08:26:02.367472  3319 net.cpp:433] ip_1 -> ip1
I1022 08:26:02.369014  3319 net.cpp:155] Setting up ip_1
I1022 08:26:02.369057  3319 net.cpp:163] Top shape: 500 20 (10000)
I1022 08:26:02.369105  3319 layer_factory.hpp:76] Creating layer relu1
I1022 08:26:02.369135  3319 net.cpp:110] Creating Layer relu1
I1022 08:26:02.369181  3319 net.cpp:477] relu1 <- ip1
I1022 08:26:02.369207  3319 net.cpp:419] relu1 -> ip1 (in-place)
I1022 08:26:02.369235  3319 net.cpp:155] Setting up relu1
I1022 08:26:02.369261  3319 net.cpp:163] Top shape: 500 20 (10000)
I1022 08:26:02.369283  3319 layer_factory.hpp:76] Creating layer ip_2
I1022 08:26:02.369312  3319 net.cpp:110] Creating Layer ip_2
I1022 08:26:02.369333  3319 net.cpp:477] ip_2 <- ip1
I1022 08:26:02.369360  3319 net.cpp:433] ip_2 -> ip2
I1022 08:26:02.369519  3319 net.cpp:155] Setting up ip_2
I1022 08:26:02.369550  3319 net.cpp:163] Top shape: 500 2 (1000)
I1022 08:26:02.369576  3319 layer_factory.hpp:76] Creating layer ip2_ip_2_0_split
I1022 08:26:02.369602  3319 net.cpp:110] Creating Layer ip2_ip_2_0_split
I1022 08:26:02.369626  3319 net.cpp:477] ip2_ip_2_0_split <- ip2
I1022 08:26:02.369652  3319 net.cpp:433] ip2_ip_2_0_split -> ip2_ip_2_0_split_0
I1022 08:26:02.369678  3319 net.cpp:433] ip2_ip_2_0_split -> ip2_ip_2_0_split_1
I1022 08:26:02.369768  3319 net.cpp:155] Setting up ip2_ip_2_0_split
I1022 08:26:02.369796  3319 net.cpp:163] Top shape: 500 2 (1000)
I1022 08:26:02.369822  3319 net.cpp:163] Top shape: 500 2 (1000)
I1022 08:26:02.369844  3319 layer_factory.hpp:76] Creating layer accuracy
I1022 08:26:02.369868  3319 net.cpp:110] Creating Layer accuracy
I1022 08:26:02.369897  3319 net.cpp:477] accuracy <- ip2_ip_2_0_split_0
I1022 08:26:02.369923  3319 net.cpp:477] accuracy <- label_nuclei_1_split_0
I1022 08:26:02.369951  3319 net.cpp:433] accuracy -> accuracy
I1022 08:26:02.369979  3319 net.cpp:155] Setting up accuracy
I1022 08:26:02.370003  3319 net.cpp:163] Top shape: (1)
I1022 08:26:02.370028  3319 layer_factory.hpp:76] Creating layer loss
I1022 08:26:02.370054  3319 net.cpp:110] Creating Layer loss
I1022 08:26:02.370079  3319 net.cpp:477] loss <- ip2_ip_2_0_split_1
I1022 08:26:02.370108  3319 net.cpp:477] loss <- label_nuclei_1_split_1
I1022 08:26:02.370136  3319 net.cpp:433] loss -> loss
I1022 08:26:02.370163  3319 layer_factory.hpp:76] Creating layer loss
I1022 08:26:02.370307  3319 net.cpp:155] Setting up loss
I1022 08:26:02.370337  3319 net.cpp:163] Top shape: (1)
I1022 08:26:02.370357  3319 net.cpp:168]     with loss weight 1
I1022 08:26:02.370390  3319 net.cpp:236] loss needs backward computation.
I1022 08:26:02.370415  3319 net.cpp:240] accuracy does not need backward computation.
I1022 08:26:02.370437  3319 net.cpp:236] ip2_ip_2_0_split needs backward computation.
I1022 08:26:02.370460  3319 net.cpp:236] ip_2 needs backward computation.
I1022 08:26:02.370482  3319 net.cpp:236] relu1 needs backward computation.
I1022 08:26:02.370504  3319 net.cpp:236] ip_1 needs backward computation.
I1022 08:26:02.370527  3319 net.cpp:236] pool2 needs backward computation.
I1022 08:26:02.370548  3319 net.cpp:236] relu2 needs backward computation.
I1022 08:26:02.370568  3319 net.cpp:236] conv2 needs backward computation.
I1022 08:26:02.370594  3319 net.cpp:236] relu1 needs backward computation.
I1022 08:26:02.370616  3319 net.cpp:236] pool1 needs backward computation.
I1022 08:26:02.370637  3319 net.cpp:236] conv1 needs backward computation.
I1022 08:26:02.370661  3319 net.cpp:240] label_nuclei_1_split does not need backward computation.
I1022 08:26:02.370683  3319 net.cpp:240] nuclei does not need backward computation.
I1022 08:26:02.370707  3319 net.cpp:283] This network produces output accuracy
I1022 08:26:02.370728  3319 net.cpp:283] This network produces output loss
I1022 08:26:02.370761  3319 net.cpp:297] Network initialization done.
I1022 08:26:02.370786  3319 net.cpp:298] Memory required for data: 146216008
I1022 08:26:02.370868  3319 solver.cpp:66] Solver scaffolding done.
I1022 08:26:02.371256  3319 caffe.cpp:128] Finetuning from examples/nuclei/train_cifar/cifar_nuclei_quick1_iter_40000.caffemodel
I1022 08:26:02.374619  3319 caffe.cpp:212] Starting Optimization
I1022 08:26:02.374698  3319 solver.cpp:294] Solving NULCEI_quick
I1022 08:26:02.374725  3319 solver.cpp:295] Learning Rate Policy: step
I1022 08:26:02.375414  3319 solver.cpp:347] Iteration 0, Testing net (#0)
I1022 08:26:02.375619  3319 blocking_queue.cpp:50] Data layer prefetch queue empty
I1022 08:26:31.409692  3319 solver.cpp:415]     Test net output #0: accuracy = 0.35368
I1022 08:26:31.409723  3319 solver.cpp:415]     Test net output #1: loss = 25.0494 (* 1 = 25.0494 loss)
I1022 08:26:31.517112  3319 solver.cpp:243] Iteration 0, loss = 21.9296
I1022 08:26:31.517154  3319 solver.cpp:259]     Train net output #0: loss = 21.9296 (* 1 = 21.9296 loss)
I1022 08:26:31.517180  3319 solver.cpp:590] Iteration 0, lr = 0.001
I1022 08:26:47.800101  3319 solver.cpp:243] Iteration 100, loss = 0.693443
I1022 08:26:47.800164  3319 solver.cpp:259]     Train net output #0: loss = 0.693445 (* 1 = 0.693445 loss)
I1022 08:26:47.800175  3319 solver.cpp:590] Iteration 100, lr = 0.001
I1022 08:27:04.074844  3319 solver.cpp:243] Iteration 200, loss = 0.691197
I1022 08:27:04.074878  3319 solver.cpp:259]     Train net output #0: loss = 0.691198 (* 1 = 0.691198 loss)
I1022 08:27:04.074887  3319 solver.cpp:590] Iteration 200, lr = 0.001
I1022 08:27:20.354104  3319 solver.cpp:243] Iteration 300, loss = 0.687065
I1022 08:27:20.354192  3319 solver.cpp:259]     Train net output #0: loss = 0.687066 (* 1 = 0.687066 loss)
I1022 08:27:20.354202  3319 solver.cpp:590] Iteration 300, lr = 0.001
I1022 08:27:36.634346  3319 solver.cpp:243] Iteration 400, loss = 0.693184
I1022 08:27:36.634379  3319 solver.cpp:259]     Train net output #0: loss = 0.693186 (* 1 = 0.693186 loss)
I1022 08:27:36.634387  3319 solver.cpp:590] Iteration 400, lr = 0.001
I1022 08:27:54.398993  3319 solver.cpp:243] Iteration 500, loss = 0.69354
I1022 08:27:54.399080  3319 solver.cpp:259]     Train net output #0: loss = 0.693541 (* 1 = 0.693541 loss)
I1022 08:27:54.399099  3319 solver.cpp:590] Iteration 500, lr = 0.001
I1022 08:31:48.728759  3319 solver.cpp:243] Iteration 600, loss = 0.694647
I1022 08:31:48.728832  3319 solver.cpp:259]     Train net output #0: loss = 0.694648 (* 1 = 0.694648 loss)
I1022 08:31:48.728842  3319 solver.cpp:590] Iteration 600, lr = 0.001
I1022 08:35:36.629190  3319 solver.cpp:243] Iteration 700, loss = 0.690552
I1022 08:35:36.629341  3319 solver.cpp:259]     Train net output #0: loss = 0.690554 (* 1 = 0.690554 loss)
I1022 08:35:36.629354  3319 solver.cpp:590] Iteration 700, lr = 0.001
I1022 08:39:24.795495  3319 solver.cpp:243] Iteration 800, loss = 0.692297
I1022 08:39:24.795626  3319 solver.cpp:259]     Train net output #0: loss = 0.692299 (* 1 = 0.692299 loss)
I1022 08:39:24.795646  3319 solver.cpp:590] Iteration 800, lr = 0.001
I1022 08:43:11.762737  3319 solver.cpp:243] Iteration 900, loss = 0.693218
I1022 08:43:11.762820  3319 solver.cpp:259]     Train net output #0: loss = 0.69322 (* 1 = 0.69322 loss)
I1022 08:43:11.762840  3319 solver.cpp:590] Iteration 900, lr = 0.001
I1022 08:46:56.367352  3319 solver.cpp:347] Iteration 1000, Testing net (#0)
I1022 08:47:25.302100  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 08:47:25.302130  3319 solver.cpp:415]     Test net output #1: loss = 0.684264 (* 1 = 0.684264 loss)
I1022 08:47:25.403584  3319 solver.cpp:243] Iteration 1000, loss = 0.690109
I1022 08:47:25.403617  3319 solver.cpp:259]     Train net output #0: loss = 0.690111 (* 1 = 0.690111 loss)
I1022 08:47:25.403626  3319 solver.cpp:590] Iteration 1000, lr = 0.001
I1022 08:47:41.627673  3319 solver.cpp:243] Iteration 1100, loss = 0.691907
I1022 08:47:41.627738  3319 solver.cpp:259]     Train net output #0: loss = 0.691909 (* 1 = 0.691909 loss)
I1022 08:47:41.627748  3319 solver.cpp:590] Iteration 1100, lr = 0.001
I1022 08:47:57.854101  3319 solver.cpp:243] Iteration 1200, loss = 0.691963
I1022 08:47:57.854135  3319 solver.cpp:259]     Train net output #0: loss = 0.691964 (* 1 = 0.691964 loss)
I1022 08:47:57.854142  3319 solver.cpp:590] Iteration 1200, lr = 0.001
I1022 08:48:14.067749  3319 solver.cpp:243] Iteration 1300, loss = 0.687041
I1022 08:48:14.067800  3319 solver.cpp:259]     Train net output #0: loss = 0.687042 (* 1 = 0.687042 loss)
I1022 08:48:14.067809  3319 solver.cpp:590] Iteration 1300, lr = 0.001
I1022 08:48:30.279256  3319 solver.cpp:243] Iteration 1400, loss = 0.692346
I1022 08:48:30.279288  3319 solver.cpp:259]     Train net output #0: loss = 0.692348 (* 1 = 0.692348 loss)
I1022 08:48:30.279295  3319 solver.cpp:590] Iteration 1400, lr = 0.001
I1022 08:48:46.509363  3319 solver.cpp:243] Iteration 1500, loss = 0.694177
I1022 08:48:46.509418  3319 solver.cpp:259]     Train net output #0: loss = 0.694178 (* 1 = 0.694178 loss)
I1022 08:48:46.509428  3319 solver.cpp:590] Iteration 1500, lr = 0.001
I1022 08:49:02.742804  3319 solver.cpp:243] Iteration 1600, loss = 0.694618
I1022 08:49:02.742836  3319 solver.cpp:259]     Train net output #0: loss = 0.69462 (* 1 = 0.69462 loss)
I1022 08:49:02.742846  3319 solver.cpp:590] Iteration 1600, lr = 0.001
I1022 08:49:18.962318  3319 solver.cpp:243] Iteration 1700, loss = 0.689661
I1022 08:49:18.962399  3319 solver.cpp:259]     Train net output #0: loss = 0.689663 (* 1 = 0.689663 loss)
I1022 08:49:18.962409  3319 solver.cpp:590] Iteration 1700, lr = 0.001
I1022 08:49:35.193078  3319 solver.cpp:243] Iteration 1800, loss = 0.691832
I1022 08:49:35.193110  3319 solver.cpp:259]     Train net output #0: loss = 0.691834 (* 1 = 0.691834 loss)
I1022 08:49:35.193120  3319 solver.cpp:590] Iteration 1800, lr = 0.001
I1022 08:49:51.428179  3319 solver.cpp:243] Iteration 1900, loss = 0.692426
I1022 08:49:51.428242  3319 solver.cpp:259]     Train net output #0: loss = 0.692427 (* 1 = 0.692427 loss)
I1022 08:49:51.428251  3319 solver.cpp:590] Iteration 1900, lr = 0.001
I1022 08:50:07.497755  3319 solver.cpp:347] Iteration 2000, Testing net (#0)
I1022 08:50:36.450975  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 08:50:36.451051  3319 solver.cpp:415]     Test net output #1: loss = 0.684136 (* 1 = 0.684136 loss)
I1022 08:50:36.552490  3319 solver.cpp:243] Iteration 2000, loss = 0.690399
I1022 08:50:36.552523  3319 solver.cpp:259]     Train net output #0: loss = 0.690401 (* 1 = 0.690401 loss)
I1022 08:50:36.552531  3319 solver.cpp:590] Iteration 2000, lr = 0.001
I1022 08:50:52.900100  3319 solver.cpp:243] Iteration 2100, loss = 0.692359
I1022 08:50:52.900133  3319 solver.cpp:259]     Train net output #0: loss = 0.692361 (* 1 = 0.692361 loss)
I1022 08:50:52.900142  3319 solver.cpp:590] Iteration 2100, lr = 0.001
I1022 08:51:09.274078  3319 solver.cpp:243] Iteration 2200, loss = 0.691578
I1022 08:51:09.274142  3319 solver.cpp:259]     Train net output #0: loss = 0.69158 (* 1 = 0.69158 loss)
I1022 08:51:09.274155  3319 solver.cpp:590] Iteration 2200, lr = 0.001
I1022 08:51:25.553680  3319 solver.cpp:243] Iteration 2300, loss = 0.687041
I1022 08:51:25.553845  3319 solver.cpp:259]     Train net output #0: loss = 0.687043 (* 1 = 0.687043 loss)
I1022 08:51:25.553907  3319 solver.cpp:590] Iteration 2300, lr = 0.001
I1022 08:51:41.836457  3319 solver.cpp:243] Iteration 2400, loss = 0.691575
I1022 08:51:41.836519  3319 solver.cpp:259]     Train net output #0: loss = 0.691576 (* 1 = 0.691576 loss)
I1022 08:51:41.836532  3319 solver.cpp:590] Iteration 2400, lr = 0.001
I1022 08:51:58.116444  3319 solver.cpp:243] Iteration 2500, loss = 0.694543
I1022 08:51:58.116478  3319 solver.cpp:259]     Train net output #0: loss = 0.694544 (* 1 = 0.694544 loss)
I1022 08:51:58.116487  3319 solver.cpp:590] Iteration 2500, lr = 0.001
I1022 08:52:14.401329  3319 solver.cpp:243] Iteration 2600, loss = 0.694893
I1022 08:52:14.401406  3319 solver.cpp:259]     Train net output #0: loss = 0.694894 (* 1 = 0.694894 loss)
I1022 08:52:14.401415  3319 solver.cpp:590] Iteration 2600, lr = 0.001
I1022 08:52:30.683346  3319 solver.cpp:243] Iteration 2700, loss = 0.689661
I1022 08:52:30.683387  3319 solver.cpp:259]     Train net output #0: loss = 0.689662 (* 1 = 0.689662 loss)
I1022 08:52:30.683395  3319 solver.cpp:590] Iteration 2700, lr = 0.001
I1022 08:52:46.971361  3319 solver.cpp:243] Iteration 2800, loss = 0.692278
I1022 08:52:46.971412  3319 solver.cpp:259]     Train net output #0: loss = 0.692279 (* 1 = 0.692279 loss)
I1022 08:52:46.971421  3319 solver.cpp:590] Iteration 2800, lr = 0.001
I1022 08:53:03.262574  3319 solver.cpp:243] Iteration 2900, loss = 0.692425
I1022 08:53:03.262606  3319 solver.cpp:259]     Train net output #0: loss = 0.692427 (* 1 = 0.692427 loss)
I1022 08:53:03.262614  3319 solver.cpp:590] Iteration 2900, lr = 0.001
I1022 08:53:19.377284  3319 solver.cpp:347] Iteration 3000, Testing net (#0)
I1022 08:53:51.187077  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 08:53:51.187144  3319 solver.cpp:415]     Test net output #1: loss = 0.684139 (* 1 = 0.684139 loss)
I1022 08:53:51.308470  3319 solver.cpp:243] Iteration 3000, loss = 0.690072
I1022 08:53:51.308507  3319 solver.cpp:259]     Train net output #0: loss = 0.690073 (* 1 = 0.690073 loss)
I1022 08:53:51.308531  3319 solver.cpp:590] Iteration 3000, lr = 0.001
I1022 08:54:08.310760  3319 solver.cpp:243] Iteration 3100, loss = 0.691453
I1022 08:54:08.310792  3319 solver.cpp:259]     Train net output #0: loss = 0.691454 (* 1 = 0.691454 loss)
I1022 08:54:08.310803  3319 solver.cpp:590] Iteration 3100, lr = 0.001
I1022 08:54:24.533520  3319 solver.cpp:243] Iteration 3200, loss = 0.691578
I1022 08:54:24.533615  3319 solver.cpp:259]     Train net output #0: loss = 0.69158 (* 1 = 0.69158 loss)
I1022 08:54:24.533628  3319 solver.cpp:590] Iteration 3200, lr = 0.001
I1022 08:54:40.801882  3319 solver.cpp:243] Iteration 3300, loss = 0.68745
I1022 08:54:40.801916  3319 solver.cpp:259]     Train net output #0: loss = 0.687452 (* 1 = 0.687452 loss)
I1022 08:54:40.801929  3319 solver.cpp:590] Iteration 3300, lr = 0.001
I1022 08:54:57.067369  3319 solver.cpp:243] Iteration 3400, loss = 0.69196
I1022 08:54:57.067425  3319 solver.cpp:259]     Train net output #0: loss = 0.691962 (* 1 = 0.691962 loss)
I1022 08:54:57.067437  3319 solver.cpp:590] Iteration 3400, lr = 0.001
I1022 08:55:13.338870  3319 solver.cpp:243] Iteration 3500, loss = 0.694907
I1022 08:55:13.338904  3319 solver.cpp:259]     Train net output #0: loss = 0.694909 (* 1 = 0.694909 loss)
I1022 08:55:13.338917  3319 solver.cpp:590] Iteration 3500, lr = 0.001
I1022 08:55:29.616550  3319 solver.cpp:243] Iteration 3600, loss = 0.695167
I1022 08:55:29.616611  3319 solver.cpp:259]     Train net output #0: loss = 0.695169 (* 1 = 0.695169 loss)
I1022 08:55:29.616618  3319 solver.cpp:590] Iteration 3600, lr = 0.001
I1022 08:55:45.886608  3319 solver.cpp:243] Iteration 3700, loss = 0.688984
I1022 08:55:45.886642  3319 solver.cpp:259]     Train net output #0: loss = 0.688986 (* 1 = 0.688986 loss)
I1022 08:55:45.886651  3319 solver.cpp:590] Iteration 3700, lr = 0.001
I1022 08:56:02.172129  3319 solver.cpp:243] Iteration 3800, loss = 0.692277
I1022 08:56:02.172190  3319 solver.cpp:259]     Train net output #0: loss = 0.692279 (* 1 = 0.692279 loss)
I1022 08:56:02.172199  3319 solver.cpp:590] Iteration 3800, lr = 0.001
I1022 08:56:18.613215  3319 solver.cpp:243] Iteration 3900, loss = 0.692699
I1022 08:56:18.613256  3319 solver.cpp:259]     Train net output #0: loss = 0.692701 (* 1 = 0.692701 loss)
I1022 08:56:18.613265  3319 solver.cpp:590] Iteration 3900, lr = 0.001
I1022 08:56:34.722893  3319 solver.cpp:347] Iteration 4000, Testing net (#0)
I1022 08:57:03.776104  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 08:57:03.776137  3319 solver.cpp:415]     Test net output #1: loss = 0.684139 (* 1 = 0.684139 loss)
I1022 08:57:03.877707  3319 solver.cpp:243] Iteration 4000, loss = 0.6904
I1022 08:57:03.877753  3319 solver.cpp:259]     Train net output #0: loss = 0.690401 (* 1 = 0.690401 loss)
I1022 08:57:03.877763  3319 solver.cpp:590] Iteration 4000, lr = 0.001
I1022 08:57:20.141878  3319 solver.cpp:243] Iteration 4100, loss = 0.692359
I1022 08:57:20.141932  3319 solver.cpp:259]     Train net output #0: loss = 0.692361 (* 1 = 0.692361 loss)
I1022 08:57:20.141940  3319 solver.cpp:590] Iteration 4100, lr = 0.001
I1022 08:57:36.405473  3319 solver.cpp:243] Iteration 4200, loss = 0.690809
I1022 08:57:36.405508  3319 solver.cpp:259]     Train net output #0: loss = 0.690811 (* 1 = 0.690811 loss)
I1022 08:57:36.405515  3319 solver.cpp:590] Iteration 4200, lr = 0.001
I1022 08:57:52.840844  3319 solver.cpp:243] Iteration 4300, loss = 0.687854
I1022 08:57:52.840895  3319 solver.cpp:259]     Train net output #0: loss = 0.687856 (* 1 = 0.687856 loss)
I1022 08:57:52.840903  3319 solver.cpp:590] Iteration 4300, lr = 0.001
I1022 08:58:09.111902  3319 solver.cpp:243] Iteration 4400, loss = 0.690803
I1022 08:58:09.111934  3319 solver.cpp:259]     Train net output #0: loss = 0.690805 (* 1 = 0.690805 loss)
I1022 08:58:09.111943  3319 solver.cpp:590] Iteration 4400, lr = 0.001
I1022 08:58:25.372041  3319 solver.cpp:243] Iteration 4500, loss = 0.694904
I1022 08:58:25.372104  3319 solver.cpp:259]     Train net output #0: loss = 0.694906 (* 1 = 0.694906 loss)
I1022 08:58:25.372112  3319 solver.cpp:590] Iteration 4500, lr = 0.001
I1022 08:58:41.636875  3319 solver.cpp:243] Iteration 4600, loss = 0.695167
I1022 08:58:41.636916  3319 solver.cpp:259]     Train net output #0: loss = 0.695168 (* 1 = 0.695168 loss)
I1022 08:58:41.636929  3319 solver.cpp:590] Iteration 4600, lr = 0.001
I1022 08:58:59.122634  3319 solver.cpp:243] Iteration 4700, loss = 0.688988
I1022 08:58:59.122725  3319 solver.cpp:259]     Train net output #0: loss = 0.68899 (* 1 = 0.68899 loss)
I1022 08:58:59.122735  3319 solver.cpp:590] Iteration 4700, lr = 0.001
I1022 08:59:16.074934  3319 solver.cpp:243] Iteration 4800, loss = 0.692277
I1022 08:59:16.074980  3319 solver.cpp:259]     Train net output #0: loss = 0.692278 (* 1 = 0.692278 loss)
I1022 08:59:16.074990  3319 solver.cpp:590] Iteration 4800, lr = 0.001
I1022 08:59:32.696609  3319 solver.cpp:243] Iteration 4900, loss = 0.692152
I1022 08:59:32.696673  3319 solver.cpp:259]     Train net output #0: loss = 0.692154 (* 1 = 0.692154 loss)
I1022 08:59:32.696683  3319 solver.cpp:590] Iteration 4900, lr = 0.001
I1022 08:59:48.729661  3319 solver.cpp:347] Iteration 5000, Testing net (#0)
I1022 09:00:17.963125  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 09:00:17.963188  3319 solver.cpp:415]     Test net output #1: loss = 0.68414 (* 1 = 0.68414 loss)
I1022 09:00:18.067119  3319 solver.cpp:243] Iteration 5000, loss = 0.691057
I1022 09:00:18.067162  3319 solver.cpp:259]     Train net output #0: loss = 0.691059 (* 1 = 0.691059 loss)
I1022 09:00:18.067173  3319 solver.cpp:590] Iteration 5000, lr = 0.001
I1022 09:00:34.351855  3319 solver.cpp:243] Iteration 5100, loss = 0.690999
I1022 09:00:34.351891  3319 solver.cpp:259]     Train net output #0: loss = 0.691001 (* 1 = 0.691001 loss)
I1022 09:00:34.351898  3319 solver.cpp:590] Iteration 5100, lr = 0.001
I1022 09:00:50.625134  3319 solver.cpp:243] Iteration 5200, loss = 0.691194
I1022 09:00:50.625186  3319 solver.cpp:259]     Train net output #0: loss = 0.691195 (* 1 = 0.691195 loss)
I1022 09:00:50.625195  3319 solver.cpp:590] Iteration 5200, lr = 0.001
I1022 09:01:06.901523  3319 solver.cpp:243] Iteration 5300, loss = 0.688264
I1022 09:01:06.901559  3319 solver.cpp:259]     Train net output #0: loss = 0.688266 (* 1 = 0.688266 loss)
I1022 09:01:06.901566  3319 solver.cpp:590] Iteration 5300, lr = 0.001
I1022 09:01:23.172948  3319 solver.cpp:243] Iteration 5400, loss = 0.69042
I1022 09:01:23.173001  3319 solver.cpp:259]     Train net output #0: loss = 0.690421 (* 1 = 0.690421 loss)
I1022 09:01:23.173009  3319 solver.cpp:590] Iteration 5400, lr = 0.001
I1022 09:01:39.452072  3319 solver.cpp:243] Iteration 5500, loss = 0.69527
I1022 09:01:39.452108  3319 solver.cpp:259]     Train net output #0: loss = 0.695272 (* 1 = 0.695272 loss)
I1022 09:01:39.452116  3319 solver.cpp:590] Iteration 5500, lr = 0.001
I1022 09:01:55.725370  3319 solver.cpp:243] Iteration 5600, loss = 0.695722
I1022 09:01:55.725419  3319 solver.cpp:259]     Train net output #0: loss = 0.695724 (* 1 = 0.695724 loss)
I1022 09:01:55.725426  3319 solver.cpp:590] Iteration 5600, lr = 0.001
I1022 09:02:11.998179  3319 solver.cpp:243] Iteration 5700, loss = 0.689665
I1022 09:02:11.998209  3319 solver.cpp:259]     Train net output #0: loss = 0.689666 (* 1 = 0.689666 loss)
I1022 09:02:11.998217  3319 solver.cpp:590] Iteration 5700, lr = 0.001
I1022 09:02:28.269214  3319 solver.cpp:243] Iteration 5800, loss = 0.692276
I1022 09:02:28.269268  3319 solver.cpp:259]     Train net output #0: loss = 0.692278 (* 1 = 0.692278 loss)
I1022 09:02:28.269278  3319 solver.cpp:590] Iteration 5800, lr = 0.001
I1022 09:02:44.551880  3319 solver.cpp:243] Iteration 5900, loss = 0.691879
I1022 09:02:44.551916  3319 solver.cpp:259]     Train net output #0: loss = 0.691881 (* 1 = 0.691881 loss)
I1022 09:02:44.551924  3319 solver.cpp:590] Iteration 5900, lr = 0.001
I1022 09:03:00.677438  3319 solver.cpp:347] Iteration 6000, Testing net (#0)
I1022 09:03:29.736523  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 09:03:29.736558  3319 solver.cpp:415]     Test net output #1: loss = 0.684138 (* 1 = 0.684138 loss)
I1022 09:03:29.842948  3319 solver.cpp:243] Iteration 6000, loss = 0.691385
I1022 09:03:29.842984  3319 solver.cpp:259]     Train net output #0: loss = 0.691387 (* 1 = 0.691387 loss)
I1022 09:03:29.842993  3319 solver.cpp:590] Iteration 6000, lr = 0.001
I1022 09:03:46.118971  3319 solver.cpp:243] Iteration 6100, loss = 0.690094
I1022 09:03:46.119050  3319 solver.cpp:259]     Train net output #0: loss = 0.690095 (* 1 = 0.690095 loss)
I1022 09:03:46.119060  3319 solver.cpp:590] Iteration 6100, lr = 0.001
I1022 09:04:02.390066  3319 solver.cpp:243] Iteration 6200, loss = 0.690426
I1022 09:04:02.390102  3319 solver.cpp:259]     Train net output #0: loss = 0.690427 (* 1 = 0.690427 loss)
I1022 09:04:02.390110  3319 solver.cpp:590] Iteration 6200, lr = 0.001
I1022 09:04:18.658447  3319 solver.cpp:243] Iteration 6300, loss = 0.688672
I1022 09:04:18.658505  3319 solver.cpp:259]     Train net output #0: loss = 0.688674 (* 1 = 0.688674 loss)
I1022 09:04:18.658516  3319 solver.cpp:590] Iteration 6300, lr = 0.001
I1022 09:04:34.921787  3319 solver.cpp:243] Iteration 6400, loss = 0.690035
I1022 09:04:34.921823  3319 solver.cpp:259]     Train net output #0: loss = 0.690036 (* 1 = 0.690036 loss)
I1022 09:04:34.921831  3319 solver.cpp:590] Iteration 6400, lr = 0.001
I1022 09:04:51.192047  3319 solver.cpp:243] Iteration 6500, loss = 0.694542
I1022 09:04:51.192111  3319 solver.cpp:259]     Train net output #0: loss = 0.694544 (* 1 = 0.694544 loss)
I1022 09:04:51.192119  3319 solver.cpp:590] Iteration 6500, lr = 0.001
I1022 09:05:07.465754  3319 solver.cpp:243] Iteration 6600, loss = 0.69545
I1022 09:05:07.465788  3319 solver.cpp:259]     Train net output #0: loss = 0.695451 (* 1 = 0.695451 loss)
I1022 09:05:07.465797  3319 solver.cpp:590] Iteration 6600, lr = 0.001
I1022 09:05:23.735900  3319 solver.cpp:243] Iteration 6700, loss = 0.690006
I1022 09:05:23.735947  3319 solver.cpp:259]     Train net output #0: loss = 0.690008 (* 1 = 0.690008 loss)
I1022 09:05:23.735959  3319 solver.cpp:590] Iteration 6700, lr = 0.001
I1022 09:05:40.019420  3319 solver.cpp:243] Iteration 6800, loss = 0.692051
I1022 09:05:40.019451  3319 solver.cpp:259]     Train net output #0: loss = 0.692053 (* 1 = 0.692053 loss)
I1022 09:05:40.019459  3319 solver.cpp:590] Iteration 6800, lr = 0.001
I1022 09:05:56.291090  3319 solver.cpp:243] Iteration 6900, loss = 0.692152
I1022 09:05:56.291141  3319 solver.cpp:259]     Train net output #0: loss = 0.692153 (* 1 = 0.692153 loss)
I1022 09:05:56.291153  3319 solver.cpp:590] Iteration 6900, lr = 0.001
I1022 09:06:12.418570  3319 solver.cpp:347] Iteration 7000, Testing net (#0)
I1022 09:06:41.475270  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 09:06:41.475325  3319 solver.cpp:415]     Test net output #1: loss = 0.684143 (* 1 = 0.684143 loss)
I1022 09:06:41.576853  3319 solver.cpp:243] Iteration 7000, loss = 0.691714
I1022 09:06:41.576887  3319 solver.cpp:259]     Train net output #0: loss = 0.691716 (* 1 = 0.691716 loss)
I1022 09:06:41.576895  3319 solver.cpp:590] Iteration 7000, lr = 0.001
I1022 09:06:57.845825  3319 solver.cpp:243] Iteration 7100, loss = 0.690093
I1022 09:06:57.845855  3319 solver.cpp:259]     Train net output #0: loss = 0.690095 (* 1 = 0.690095 loss)
I1022 09:06:57.845862  3319 solver.cpp:590] Iteration 7100, lr = 0.001
I1022 09:07:14.117815  3319 solver.cpp:243] Iteration 7200, loss = 0.690426
I1022 09:07:14.117867  3319 solver.cpp:259]     Train net output #0: loss = 0.690428 (* 1 = 0.690428 loss)
I1022 09:07:14.117880  3319 solver.cpp:590] Iteration 7200, lr = 0.001
I1022 09:07:30.392087  3319 solver.cpp:243] Iteration 7300, loss = 0.689486
I1022 09:07:30.392122  3319 solver.cpp:259]     Train net output #0: loss = 0.689488 (* 1 = 0.689488 loss)
I1022 09:07:30.392128  3319 solver.cpp:590] Iteration 7300, lr = 0.001
I1022 09:07:46.668251  3319 solver.cpp:243] Iteration 7400, loss = 0.690421
I1022 09:07:46.668318  3319 solver.cpp:259]     Train net output #0: loss = 0.690422 (* 1 = 0.690422 loss)
I1022 09:07:46.668329  3319 solver.cpp:590] Iteration 7400, lr = 0.001
I1022 09:08:02.953037  3319 solver.cpp:243] Iteration 7500, loss = 0.694177
I1022 09:08:02.953073  3319 solver.cpp:259]     Train net output #0: loss = 0.694179 (* 1 = 0.694179 loss)
I1022 09:08:02.953083  3319 solver.cpp:590] Iteration 7500, lr = 0.001
I1022 09:08:19.235934  3319 solver.cpp:243] Iteration 7600, loss = 0.694622
I1022 09:08:19.236017  3319 solver.cpp:259]     Train net output #0: loss = 0.694624 (* 1 = 0.694624 loss)
I1022 09:08:19.236028  3319 solver.cpp:590] Iteration 7600, lr = 0.001
I1022 09:08:35.503804  3319 solver.cpp:243] Iteration 7700, loss = 0.689667
I1022 09:08:35.503840  3319 solver.cpp:259]     Train net output #0: loss = 0.689668 (* 1 = 0.689668 loss)
I1022 09:08:35.503849  3319 solver.cpp:590] Iteration 7700, lr = 0.001
I1022 09:08:51.771329  3319 solver.cpp:243] Iteration 7800, loss = 0.692275
I1022 09:08:51.771399  3319 solver.cpp:259]     Train net output #0: loss = 0.692277 (* 1 = 0.692277 loss)
I1022 09:08:51.771409  3319 solver.cpp:590] Iteration 7800, lr = 0.001
I1022 09:09:08.046491  3319 solver.cpp:243] Iteration 7900, loss = 0.692426
I1022 09:09:08.046521  3319 solver.cpp:259]     Train net output #0: loss = 0.692427 (* 1 = 0.692427 loss)
I1022 09:09:08.046530  3319 solver.cpp:590] Iteration 7900, lr = 0.001
I1022 09:09:24.160673  3319 solver.cpp:347] Iteration 8000, Testing net (#0)
I1022 09:09:53.200038  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 09:09:53.200073  3319 solver.cpp:415]     Test net output #1: loss = 0.68414 (* 1 = 0.68414 loss)
I1022 09:09:53.304440  3319 solver.cpp:243] Iteration 8000, loss = 0.693028
I1022 09:09:53.304476  3319 solver.cpp:259]     Train net output #0: loss = 0.69303 (* 1 = 0.69303 loss)
I1022 09:09:53.304486  3319 solver.cpp:590] Iteration 8000, lr = 0.001
I1022 09:10:09.651433  3319 solver.cpp:243] Iteration 8100, loss = 0.690545
I1022 09:10:09.651485  3319 solver.cpp:259]     Train net output #0: loss = 0.690547 (* 1 = 0.690547 loss)
I1022 09:10:09.651499  3319 solver.cpp:590] Iteration 8100, lr = 0.001
I1022 09:10:25.868746  3319 solver.cpp:243] Iteration 8200, loss = 0.691193
I1022 09:10:25.868778  3319 solver.cpp:259]     Train net output #0: loss = 0.691195 (* 1 = 0.691195 loss)
I1022 09:10:25.868789  3319 solver.cpp:590] Iteration 8200, lr = 0.001
I1022 09:10:42.080695  3319 solver.cpp:243] Iteration 8300, loss = 0.689896
I1022 09:10:42.080749  3319 solver.cpp:259]     Train net output #0: loss = 0.689898 (* 1 = 0.689898 loss)
I1022 09:10:42.080761  3319 solver.cpp:590] Iteration 8300, lr = 0.001
I1022 09:10:58.305032  3319 solver.cpp:243] Iteration 8400, loss = 0.690421
I1022 09:10:58.305064  3319 solver.cpp:259]     Train net output #0: loss = 0.690423 (* 1 = 0.690423 loss)
I1022 09:10:58.305081  3319 solver.cpp:590] Iteration 8400, lr = 0.001
I1022 09:11:14.533617  3319 solver.cpp:243] Iteration 8500, loss = 0.694176
I1022 09:11:14.533682  3319 solver.cpp:259]     Train net output #0: loss = 0.694178 (* 1 = 0.694178 loss)
I1022 09:11:14.533691  3319 solver.cpp:590] Iteration 8500, lr = 0.001
I1022 09:11:30.773671  3319 solver.cpp:243] Iteration 8600, loss = 0.695172
I1022 09:11:30.773705  3319 solver.cpp:259]     Train net output #0: loss = 0.695174 (* 1 = 0.695174 loss)
I1022 09:11:30.773715  3319 solver.cpp:590] Iteration 8600, lr = 0.001
I1022 09:11:46.991585  3319 solver.cpp:243] Iteration 8700, loss = 0.689667
I1022 09:11:46.991647  3319 solver.cpp:259]     Train net output #0: loss = 0.689669 (* 1 = 0.689669 loss)
I1022 09:11:46.991655  3319 solver.cpp:590] Iteration 8700, lr = 0.001
I1022 09:12:03.229467  3319 solver.cpp:243] Iteration 8800, loss = 0.692275
I1022 09:12:03.229501  3319 solver.cpp:259]     Train net output #0: loss = 0.692276 (* 1 = 0.692276 loss)
I1022 09:12:03.229511  3319 solver.cpp:590] Iteration 8800, lr = 0.001
I1022 09:12:19.447583  3319 solver.cpp:243] Iteration 8900, loss = 0.692973
I1022 09:12:19.447634  3319 solver.cpp:259]     Train net output #0: loss = 0.692974 (* 1 = 0.692974 loss)
I1022 09:12:19.447648  3319 solver.cpp:590] Iteration 8900, lr = 0.001
I1022 09:12:35.511339  3319 solver.cpp:347] Iteration 9000, Testing net (#0)
I1022 09:13:04.472957  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 09:13:04.473048  3319 solver.cpp:415]     Test net output #1: loss = 0.684133 (* 1 = 0.684133 loss)
I1022 09:13:04.572721  3319 solver.cpp:243] Iteration 9000, loss = 0.693028
I1022 09:13:04.572754  3319 solver.cpp:259]     Train net output #0: loss = 0.69303 (* 1 = 0.69303 loss)
I1022 09:13:04.572763  3319 solver.cpp:590] Iteration 9000, lr = 0.001
I1022 09:13:20.799223  3319 solver.cpp:243] Iteration 9100, loss = 0.691906
I1022 09:13:20.799253  3319 solver.cpp:259]     Train net output #0: loss = 0.691908 (* 1 = 0.691908 loss)
I1022 09:13:20.799262  3319 solver.cpp:590] Iteration 9100, lr = 0.001
I1022 09:13:37.022759  3319 solver.cpp:243] Iteration 9200, loss = 0.691194
I1022 09:13:37.022815  3319 solver.cpp:259]     Train net output #0: loss = 0.691195 (* 1 = 0.691195 loss)
I1022 09:13:37.022825  3319 solver.cpp:590] Iteration 9200, lr = 0.001
I1022 09:13:53.244468  3319 solver.cpp:243] Iteration 9300, loss = 0.690307
I1022 09:13:53.244501  3319 solver.cpp:259]     Train net output #0: loss = 0.690309 (* 1 = 0.690309 loss)
I1022 09:13:53.244511  3319 solver.cpp:590] Iteration 9300, lr = 0.001
I1022 09:14:09.467906  3319 solver.cpp:243] Iteration 9400, loss = 0.69042
I1022 09:14:09.467959  3319 solver.cpp:259]     Train net output #0: loss = 0.690422 (* 1 = 0.690422 loss)
I1022 09:14:09.467969  3319 solver.cpp:590] Iteration 9400, lr = 0.001
I1022 09:14:25.698170  3319 solver.cpp:243] Iteration 9500, loss = 0.694545
I1022 09:14:25.698210  3319 solver.cpp:259]     Train net output #0: loss = 0.694546 (* 1 = 0.694546 loss)
I1022 09:14:25.698217  3319 solver.cpp:590] Iteration 9500, lr = 0.001
I1022 09:14:41.914778  3319 solver.cpp:243] Iteration 9600, loss = 0.69462
I1022 09:14:41.914831  3319 solver.cpp:259]     Train net output #0: loss = 0.694622 (* 1 = 0.694622 loss)
I1022 09:14:41.914840  3319 solver.cpp:590] Iteration 9600, lr = 0.001
I1022 09:14:58.136857  3319 solver.cpp:243] Iteration 9700, loss = 0.689331
I1022 09:14:58.136890  3319 solver.cpp:259]     Train net output #0: loss = 0.689333 (* 1 = 0.689333 loss)
I1022 09:14:58.136898  3319 solver.cpp:590] Iteration 9700, lr = 0.001
I1022 09:15:14.363087  3319 solver.cpp:243] Iteration 9800, loss = 0.692499
I1022 09:15:14.363147  3319 solver.cpp:259]     Train net output #0: loss = 0.6925 (* 1 = 0.6925 loss)
I1022 09:15:14.363157  3319 solver.cpp:590] Iteration 9800, lr = 0.001
I1022 09:15:30.583835  3319 solver.cpp:243] Iteration 9900, loss = 0.692699
I1022 09:15:30.583868  3319 solver.cpp:259]     Train net output #0: loss = 0.692701 (* 1 = 0.692701 loss)
I1022 09:15:30.583876  3319 solver.cpp:590] Iteration 9900, lr = 0.001
I1022 09:15:46.649626  3319 solver.cpp:468] Snapshotting to binary proto file examples/nuclei/train_cifar/use_cifar_4_iter_10000.caffemodel
I1022 09:15:46.736124  3319 solver.cpp:753] Snapshotting solver state to binary proto file examples/nuclei/train_cifar/use_cifar_4_iter_10000.solverstate
I1022 09:15:46.736659  3319 solver.cpp:347] Iteration 10000, Testing net (#0)
I1022 09:16:15.614785  3319 solver.cpp:415]     Test net output #0: accuracy = 0.64532
I1022 09:16:15.614816  3319 solver.cpp:415]     Test net output #1: loss = 0.684127 (* 1 = 0.684127 loss)
I1022 09:16:15.714475  3319 solver.cpp:243] Iteration 10000, loss = 0.693357
I1022 09:16:15.714509  3319 solver.cpp:259]     Train net output #0: loss = 0.693359 (* 1 = 0.693359 loss)
I1022 09:16:15.714517  3319 solver.cpp:590] Iteration 10000, lr = 0.001
I1022 09:16:32.062896  3319 solver.cpp:243] Iteration 10100, loss = 0.690999
I1022 09:16:32.062948  3319 solver.cpp:259]     Train net output #0: loss = 0.691001 (* 1 = 0.691001 loss)
I1022 09:16:32.062958  3319 solver.cpp:590] Iteration 10100, lr = 0.001
I1022 09:16:48.335350  3319 solver.cpp:243] Iteration 10200, loss = 0.691193
I1022 09:16:48.335386  3319 solver.cpp:259]     Train net output #0: loss = 0.691195 (* 1 = 0.691195 loss)
I1022 09:16:48.335398  3319 solver.cpp:590] Iteration 10200, lr = 0.001
